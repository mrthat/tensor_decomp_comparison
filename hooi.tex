\section{Higher-order Orthogonal Iteration (HOOI)}
\label{sec_hooi}

Next algorithm is Higher-Order Orthogonal Iteration (HOOI). (see~\cite{kolda_bader_2009}).
This algorithm is a special case of alternating least squares.
In one iteration of this method we fix $F_2$ and optimize the cost function for the other factor matrix $F_3$,
then fix $F_3$ and solve the optimization problem for $F_2$. 
The advantage of this procedure is that optimization for one factor matrix can be solved exactly
by SVD, as mentioned earlier. The pseudocode form of the algorithm is

\begin{algorithm}
\caption{HOOI}\label{HOOI}
\begin{algorithmic}[1]
\Function{HOOI}{ $\A \in \R^{d_1 \times d_2 \times d_3}$, $r_2$, $r_3$}

\State Initialize $F_2$, $F_3$ using HOSVD
\Repeat 

$\mathcal{B} := \A \times{_3} F_3^T$
    
    
$\mathcal{C} := \A \times{_2} F_2^T$
    
    
$F_2 := d_2 \mbox{ leading left singular vectors of }{\mathcal{B}}_{(2)}$
    
    
    $F_3 := d_3 \mbox{ leading left singular vectors of }{\mathcal{C}}_{(3)}$
\Until termination criterion satisfied or maximum iterations exhausted.


\State \Return $F_2$, $F_3$


\EndFunction
\end{algorithmic}
\end{algorithm}


There are several standard options for termination criterion.
We may require that the norm of the residual is below
some predefined threshold,
\begin{equation}
    R := \| \A - \A \times_2 F_2^T \times_3 F_3^T \| \leq \epsilon,
\end{equation} 
or we may consider the relative norm,

\begin{equation}
    \frac{R}{\| \A \| } \leq \epsilon.
\end{equation} 


Another option would be to stop when the norm or the relative norm of the gradient of the objective
function $\Phi$ gets small enough,
\begin{eqnarray}
    \| \nabla \Phi \| \leq \epsilon \mbox{ or}\\
    \frac{ \| \nabla \Phi \|  }{\| \A \| } \leq \epsilon.
\end{eqnarray} 
The compact formula for the gradient will be given in the section on Newton-Grassmann
method (formula \eqref{grad_tucker2}). It is important to use the projected
gradient here, the formal gradient (vector of partial
derivatives with respect to the elements of $F_2$ and $F_3$, its compact
representation is given in the formula \eqref{formal_grad_phi})
increases as we run the iterations, due to the 
orthogonality constraints.
It is also possible to stop iterations when there is no further improvement. 
For example, let $R_k$ be the norm of the residual
at the {$k$th} iteration. We keep iterating as long as  $R_k \leq R_{k-1} - \epsilon$.



It is important to mention here that both Newton methods we consider
are not monotonic, so it is not sufficient to compare the current result
with the result of the previous iteration. It is safe to stop
iterating only if we have seen several iterations that did not improve.
