\section{Newton-Grassmann method}
\label{sec_newgr}

This method was introduced by Elden and Savas. 
The general scheme of the method is as follows.
Let $\Phi \colon M \to \R$ be a smooth function 
on a Riemannian manifold $M$, we want to maximize $\Phi$.
Let $p \in M$
be a current approximation. The function $\Phi$ has the gradient 
$\nabla \Phi (p) \in T_pM$ and the Hessian $H(\Phi)_p$
which can be interpreted as a linear operator on $T_pM$.
If $p$ is close enough to the point of maximum of $\Phi$,
the Hessian is non-singular, and there exists a unique tangent vector 
$\Delta \in T_pM$ that satisfies the equation
\begin{equation}
    \label{NewgrGeneral}
    H(\Phi)_p(\Delta) = - \nabla \Phi(p).
\end{equation}
This vector defines the geodesic curve $t \mapsto \gamma(t)$ on $M$,
and we move along it to find the next approximation to the solution,
\begin{equation}
p_{+} = \gamma(1).
\end{equation}


The manifold on which the method by Elden and Savas works is the product
of \textit{Grassmann manifolds}, and we start with its definition and basic properties.  Let $k \leq n$ be positive integers. We interpret the manifold $St(n, k)$
of $n \times k$ matrices with orthonormal columns
as the set of orthonormal basises of $k$-dimensional subspaces
of $\R^n$. Two matrices $X$ and $Y$ represent the same subspace,
if there exists an orthogonal $k \times k$ matrix $U$ such that
$X = YU$. This defines an equivalence relation $\sim$ on $St(n, k)$. The quotient $St(n, k)/ \sim$ is called
Grassmann manifold and denoted $Gr(n, k)$.
Let $[X] \in Gr(n,k)$ be an equivalence class of matrix $X$.
It is known that the tangent
space $T_{[X]}Gr(n, k)$ can be identified with the set of matrices $\Delta$
such that 
\begin{equation}
    X^T \Delta = 0
\end{equation}
Let $\Pi$ be an operator
\begin{equation}
    \Pi_X(W) = W - XX^TW
\end{equation}
This operator projects $\R^{n \times k}$ onto $T_XGr(n, ,k)$. Indeed,

\begin{equation}
    X^T \Pi_X(W) = X^T W - X^TXX^T W = X^TW - X^TW = 0,
\end{equation}
since $X$ has orthonormal columns, that is, $X^TX = I$.


The canonical inner product in $T_X Gr(n, k)$ is defined by
\begin{equation}
    \langle \Delta_1, \Delta_2 \rangle := \tr( \Delta_1^T \Delta_2).
\end{equation}
Equipped with this inner product, $Gr(n, k)$ becomes a Riemannian
manifold. The formula
for the geodesic curve that passes through $X$ at $t = 0$ and has at this point tangent 
vector $\Delta$ is 
\begin{equation}
    \label{geodesic_grassm}
    X_{\Delta}(t) = XV \cos (t \Sigma) V^T + U \sin(t \Sigma) V^T,
\end{equation}
where $U \Sigma V^T$ is the thin SVD of $\Delta$.


In the sequel we shall abuse the notation by identifying $[X] \in Gr(n, k)$
and its representative $X \in \R^{n \times k}$. Functions $F$
on $Gr(n, k)$ will be interpreted as functions on $St(n, k)$ with the additional
symmetry property
\begin{equation}
    \label{sym_grass}
    \forall U \in O(k) \quad F(XU) = F(X).
\end{equation}
Since $St(n,k)$ can be identified with the subset of $\R^{nk}$, we can compute
the \textit{formal gradient} of $F$ by taking partial derivatives of $F$
with respect to all entries of the argument $X$. Let this gradient
be $F_X(X)$, then the gradient of $F$ as a function on Grassmannian
is computed by projecting:
\begin{equation}
\nabla F (X) = \Pi_{X} F_X(X).
\end{equation}

We noticed before that the objective function in Tucker2 decomposition 
problem
\begin{equation}
\Phi(F_2,F_3) = \frac{1}{2} \| \A \times _{2} F_2 \times _{3} F_3 \| 
\end{equation}
satisfies the symmetry property \eqref{sym_grass} w.r.t. both arguments,
so we can interpret it as a function on $M = Gr(d_2, r_2) \times Gr(d_3, r_3)$.
We start with computing its gradient. 

%Recall that for any function $f$ the linear part of $ f(x(t)) - f(x(0)) $ is equal to $ \langle \Delta, \nabla f \rangle $,
%where $\Delta$ is the tangent vector to the curve $x(\cdot)$ at $0$. We shall use this property 
%to calculate the gradient of $\Phi$.

Let ${F_2}(t)$ and ${F_3}(t)$ be arbitrary curves on $Gr(d_2, r_2)$
and $Gr(d_3, r_3)$, $\Delta_2$ and $\Delta_3$ be the corresponding tangent
vectors at $t = 0$.
If we derive the representation of $\frac{d \Phi(F_2(t), F_3(t))}{dt} \rvert_{t = 0}$ in the form
$\langle \Delta, \Theta \rangle$, where $\Delta$ is a tangent vector
to the curve $t \mapsto (F_2(t), F_3(t))$ on $St(d_2, r_2) \times St(d_3, r_3)$ 
at $t = 0$, then the vector $\Theta$ will be the formal gradient of $\Phi$.
Since the tangent space to the product of two Riemannian manifolds is
isomorphic to the product of tangent spaces (as inner product spaces), we
have $\Delta = (\Delta_2, \Delta_3)$, $\Theta = (\Theta_2, \Theta_3)$
and $\langle \Delta, \Theta \rangle = \langle \Delta_2, \Theta_2 \rangle + 
\langle \Delta_3, \Theta_3 \rangle $.


%\begin{equation}
%\frac{d}{dt}\left(A \times _k X(t) \times _m {F_2}(t) \right) = A \times_k \frac{d}{dt}(X(t)) \times _m {F_2}(t) + A \times_k  X(t) \times _m \frac{d}{dt}({F_2}(t))
%\end{equation}
%Here $A$ is a constant tensor and $X(t)$ and ${F_2}(t)$ are matrix-valued functions of $A$.

Using the Leibniz formula, we obtain
\begin{equation}
\begin{split}
\frac{d\Phi(F_2(t), F_3(t))} {dt} &= \frac{1}{2} \frac{d}{dt} \langle \A \times_2 F_2(t) \times_3 F_3(t), \A \times_2 F_2(t) \times_3 F_3(t) \rangle   \\
 &= \langle \A \times_2 \Delta_2 \times_3 F_3, \A \times_2 F_2 \times_3 F_3 \rangle  \\ 
 & +  \langle \A \times_2 F_2 \times_3 \Delta_3 , \A \times_2 F_2 \times_3 F_3 \rangle 
 \end{split}
\end{equation}

Let
\begin{equation}
\G := \A \times_2 {F_2} \times_3 {F_3}.
\end{equation}

Using formulae from \cite{elden_savas_2009}, we get

\begin{eqnarray}
\langle \A \times_2 \Delta{_2} \times_3 {F_3}, F \rangle = \langle \Delta{_2}, \langle  \A \times_3 {F_3}, F \rangle _{-2} \rangle, \\
\langle \A \times_2 {F_2} \times_3 \Delta{_3}, F \rangle = \langle \Delta{_3}, \langle  \A \times_2 {F_2}, F \rangle _{-3} \rangle.
\end{eqnarray}

This shows that the formal gradient of $\Phi$ consists of two components $(\Phi_2, \Phi_3)$
given by

\begin{eqnarray}
\label{formal_grad_phi}
\Phi{_2} := \langle  \A \times_3 {F_3}, F \rangle _{-2} \\
\Phi{_3} := \langle  \A \times_2 {F_2}, F \rangle _{-3}
\end{eqnarray}

Now we need to project to get $\Phi$, the $\Phi_2$ component is projected to $T_{F_2}Gr(d_2, r_2)$ and
the $\Phi_3$ component is projected to $T_{F_3}Gr(d_3, r_3)$. Omitting the intermediate calculations,
we give the final result ($\Pi_i$ stands for $\Pi_{F_i}$ ):

\begin{equation}
\label{grad_tucker2}
\nabla \Phi = 
\begin{pmatrix}
\Pi{_2} \Phi{_2} \\
\Pi{_3} \Phi{_3}
\end{pmatrix} 
=
\begin{pmatrix}
\langle \A \times_3 {F_3}, \A \times_3 {F_3} \rangle_{-2}{F_2} - {F_2} \langle F, F \rangle_{-2} \\
\langle \A \times_2 {F_2}, \A \times_2 {F_2} \rangle_{-3} {F_3} - {F_3} \langle F, F \rangle_{-3}
\end{pmatrix}
\end{equation}

Next step is to compute the Hessian $H$. Since $\Phi$ is defined on the product
of two manifolds, the Hessian $H$ has the block structure
\begin{equation}
    H = \begin{pmatrix}
        H_{2,2} & H_{2,3} \\
        H_{3,2} & H_{3,3} \\
    \end{pmatrix}
\end{equation}
Here $H_{i,j}$ is a linear operator from $T_{F_j}Gr(d_j, r_j)$ to $T_{F_i}Gr(d_i, r_i)$.
Since $H$ is a self-adjoint operator, $H_{2,3} = H_{3,2}'$.


We again start from computing Hessian
w.r.t. elements of $F_2$ and $F_3$, then we shall need to apply projection 
operator to get $H$. From standard calculus in $\R^n$ we know that
Hessian can be characterized as the linear operator $H$ such that
\begin{equation}
\frac{d^2 \Phi(F_2(t), F_3(t))}{dt} \rvert_{t = 0} = \langle \Delta, H \Delta \rangle.
\end{equation}
Calculate the second derivative of $\Phi(F_2(t), F_3(t))$:
\begin{eqnarray*}
\frac{d^2 \Phi}{dt^2} = \langle \A \times_2 \Delta{_2} \times_3 {F_3}, \A \times_2 \Delta{_2} \times_3 {F_3} \rangle 
+ \langle \A \times_2 \Delta{_2} \times_3 {F_3}, \A \times_2 {F_2} \times_3 \Delta{_3} \rangle \\
+ \langle \A \times_2 \Delta{_2} \times_3 \Delta{_3}, F \rangle 
+ \langle \A \times_2 {F_2} \times_3 \Delta{_3}, \A \times_2 \Delta{_2} \times_3 {F_3} \rangle \\ 
+ \langle \A \times_2 {F_2} \times_3 \Delta{_3}, \A \times_2 {F_2}  \times_3 \Delta_{F_3} \rangle 
+ \langle \A \times_2 \Delta{_2} \times_3 \Delta{_3}, F \rangle 
\end{eqnarray*}

Consider the $(2,2)$-term:

\begin{eqnarray*}
\langle \A \times_2 \Delta{_2} \times_3 {F_3}, \A \times_2 \Delta{_2} \times_3 {F_3} \rangle \\
= \langle \Delta{_2}, \langle \A \times_3 {F_3}, \A \times_2 \Delta{_2} \times_3 {F_3} \rangle_{-2} \rangle \\
 = \langle \Delta{_2}, \langle \A \times_3 {F_3}, \A  \times_3 {F_3} \rangle_{-2} \Delta{_2} \rangle
\end{eqnarray*}

This gives the $(2,2)$-term of the Hessian in operator form:

\begin{equation}
H_{2,2}(\Delta{_2}) = \Pi{_2} \langle \A \times_3 {F_3}, \A \times_3 {F_3} \rangle_{-2} \Delta{_2} - \Delta{_2} {F_2}^T \Phi{_2}
\end{equation}

In the same way the $(3,3)$-term is

\begin{equation}
{H}_{3,3}(\Delta{_3}) = \Pi{_3} \langle \A \times_2 {F_2}, \A \times_2 {F_2} \rangle_{-3} \Delta{_3} - \Delta{_3} {F_3}^T \Phi{_2}
\end{equation}

The $(2,3)$-term is a bit more tricky to compute.

\begin{equation}
H_{2,3}(\Delta{_3}) = \Pi{_2} \left( \langle  \langle \A, F \rangle_{-(2,3)}, \Delta{_3} \rangle_{2,4;1,2}    
+ \langle \langle \A \times_3 {F_3}, \A \times_2 {F_2} \rangle_{-(2,3)}, \Delta{_3} \rangle_{4,2;1:2} \right)
\end{equation}

Until now it was sufficient to work with $T_F Gr(d,r)$ as a subspace of  $\R^{d \times r}$.
However, the elements of $\Delta_2$, $\Delta_3$ are not linearly independent:
the condition $F_i^T \Delta_i = 0$ imposes additional linear constraints,
so it would be unwise to use the elements of these matrices as unknowns
in the final system of equations. We want a non-singular system
with the same number of equations and unknowns. There is a more explicit description
of $T_F Gr(d, r)$ that allows us to do this.  For a $d \times r$ ($d > r$) matrix
$F$ with orthonormal columns we can always find a (non-unique) matrix $F^{\bot}$ such that $[ F \,\, F_{\bot} ]$
is an orthogonal $d \times d$ matrix (in more geometric terms, we extend the orthonormal
basis of the subspace to the ortonormal basis of the whole space). It turns out that
$\Delta = F^{\bot} D$, $D \in \R^{(d-r) \times r}$ is the appropriate parametrization
of $T_F Gr(d, r)$. 
Even though $F^{\bot}$ is not unique, the projection onto the tangent space
can expressed in terms of $F^{\bot}$ as
\begin{equation}
    \Pi_F = F^{\bot} (F^{\bot})^T.
\end{equation}

Let us fix some
$F_{2}^{\bot}$ and $F_3^{\bot}$. Then, for $D_i \in \R^{(d_i - r_i) \times r_i}$
we have
\begin{eqnarray}
    \label{delta_to_d}
\Delta{_2} = {F_2}^{\bot} D{_2} \\
\Delta{_3} = {F_3}^{\bot} D{_3}
\end{eqnarray}

We can substitute these expressions
for $\Delta_i$ into the formulas for $\nabla \Phi$ and $H(\Phi)$
and this yields the final formulas for the matrix of the system
and its right-hand side.
$F_{2, \bot}$
Let us start with the gradient. If we work in the tangent space, we have
\begin{eqnarray}
    \langle \Delta_2, \Pi_{F_2} \Phi_2 \rangle & = \langle F_2^{\bot} D_y, F_2^{\bot} ( F_2^{\bot} )^T \Phi_2 \rangle \\
    & = \langle D_y,( F_2^{\bot})^T  F_2^{\bot} ( F_2^{\bot} )^T \Phi_2 \rangle  \\
    & = \langle D_y, (F_2^{\bot}) ^T \Phi_2 \rangle 
\end{eqnarray}
The last equality holds, because $F_2^{\bot}$ has orthonormal columns, 
so $( F_2^{\bot})^T  F_2^{\bot}$ is the identity matrix.



%\begin{equation}
%\hat{\mathcal{H}}(D) = \begin{pmatrix}
%{F_2}_{\bot}^T  & 0 \\
%0 & {F_3}^T_{\bot} 
%\end{pmatrix} \begin{pmatrix}
%\mathcal{H}_{yy}(\Delta{_2}) + \mathcal{H}_{2,3}(\Delta{_3}) \\
%\mathcal{H}_{zy}(\Delta{_2}) + \mathcal{H}_{zz}(\Delta{_3})
%\end{pmatrix}
%\end{equation}

\begin{equation}
\hat{{H}}_{2,2}(D{_2}) = \langle \hat{B}{_2}, \hat{B}{_2} \rangle_{-2} D{_2} - D{_2} \langle F, F \rangle _{-2}
\end{equation}

\begin{equation}
\hat{{H}}_{3,3}(D{_3}) = \langle \hat{B}{_3}, \hat{B}{_3} \rangle_{-3} D{_3} - D{_3} \langle F, F \rangle _{-3}
\end{equation}

Here
\begin{eqnarray}
\hat{B}{_2} = \A \times_2 {F_2}_{\bot} \times_3 {F_3}
\hat{B}{_3} = \A \times_2 {F_2} \times_3 {F_3}_{\bot}
\end{eqnarray}


\begin{equation}
\hat{{H}}_{2,3}(D{_3}) = \langle \langle \hat{C}_{2,3}, F \rangle_{-(2,3)} D{_3} \rangle_{2,4;1:2} + \langle \langle \hat{B}_{2}, \hat{B}_{2,3} \rangle_{-(2,3)} D{_3} \rangle_{4,2;1:2}
\end{equation}
Here
\begin{equation}
    \hat{C}_{2,3} = \A \times_2 F_{2 \bot} \times_3 F_{3 \bot}
\end{equation}

The gradient expressed in terms of $D_2$, $D_3$ is
\begin{equation}
\nabla \hat{\Phi} = \begin{pmatrix}
{F_2}_{\bot}^T & 0 \\
0 & {F_3}^T_{\bot} 
\end{pmatrix} \begin{pmatrix}
\Pi_{{F_2}}\Phi{_2} \\
\Pi_{F_3} \Phi{_3}
\end{pmatrix}
\end{equation}




We now have the linear system 
\begin{equation}
\hat{{H}}(D{_2}, D{_3}) = -\nabla \hat{\Phi}
\end{equation}
with unknown matrices $D{_2}$ and $D{_3}$. In order to solve this system
we need to vectorize it.



\begin{equation}
    \label{ng_system_fin}
    \hat{H}_{v} d = - g
\end{equation}



The standard rules for vectorization are
\begin{eqnarray*}
\vect(AX) = (I \otimes A) \vect(X) \\
\vect(XA) = (A \otimes I) \vect(X)
\end{eqnarray*}

These rules are applicable to the diagonal elements of $\hat{\mathcal{H}}$. The offidiagonal element
is of the form
\begin{equation}
\langle A, X \rangle _{2,4;1:2} + \langle B, X \rangle_{4,2;1:2}
\end{equation}
It can be verified that the vectorization of this operator is
\begin{equation}
\A^{(1,3;2,4)} \vect{X} + \A^{(1,3;4,2)} \vect{X}
\end{equation}



\begin{algorithm}
\caption{Newton-Grassmann algorithm}\label{newgr_algo}
\begin{algorithmic}[1]
\Function{NewtonGrassmann}{ $\A \in \R^{d_1 \times d_2 \times d_3}$, $r_2$, $r_3$}

\State $F_2,F_3 := HOOI(\A, r_2, r_3) $ 
\Repeat 

\State Calculate the gradient $g$ using \ref{grad_ng_for_system}
\State Calculate the Hessian $\hat{H}$ using formula \ref{hess_ng_for_system}
\State Solve the system \ref{ng_system_fin} and reshape its solution to get $D_2$, $D_3$.
\State Calculate $\Delta_2$ and $\Delta_3$ using formulas \ref{delta_to_d}.
\State Move along the geodesic curves defined by $F_i$ and $\Delta_i$ using formulas
\ref{geodesic_grassm} to get new $F_2$ and $F_3$.
    
\Until termination criterion satisfied or maximum iterations exhausted.


\State \Return $F_2$, $F_3$


\EndFunction
\end{algorithmic}
\end{algorithm}


